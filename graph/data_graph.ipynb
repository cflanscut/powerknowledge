{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b5f9473948cb6cc6acefe1e4d7af67cc2b95399d24746c153a73ff8f3dd5bf7d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import product"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 读取数据\n",
    "sys.path.append('/home/chaofan/powerknowledge/data')\n",
    "from read_PLAID_data import read_processed_data\n",
    "\n",
    "start_reading_time = time.time()\n",
    "\n",
    "feature_select = [\n",
    "    'i_mean', 'i_wave_factor', 'i_pp_rms', 'i_thd', 'pure_thd', 'P', 'Q',\n",
    "    'P_F', 'i_hp1', 'z_hp1', 'i_hm2', 'z_hm2', 'i_hp2', 'z_hp2', 'i_hm3',\n",
    "    'z_hm3', 'i_hp3', 'z_hp3', 'i_hm4', 'z_hm4', 'i_hp4', 'z_hp4', 'i_hm5',\n",
    "    'z_hm5', 'i_hp5', 'z_hp5', 'i_hm6', 'z_hm6', 'i_hp6', 'z_hp6', 'i_hm7',\n",
    "    'z_hm7', 'i_hp7', 'z_hp7'\n",
    "]  # 选择所用特征量\n",
    "\n",
    "selected_label = [\n",
    "    'Air Conditioner', 'Blender', 'Coffee maker', 'Fan', 'Fridge', 'Hair Iron',\n",
    "    'Hairdryer', 'Heater', 'Incandescent Light Bulb', 'Microwave',\n",
    "    'Soldering Iron', 'Vacuum', 'Washing Machine', 'Water kettle'\n",
    "]  # 选择所用电器\n",
    "\n",
    "load_transformer = {}\n",
    "num = 0\n",
    "for item in selected_label:\n",
    "    load_transformer[item] = num\n",
    "    num += 1\n",
    "\n",
    "each_file_len = 20\n",
    "# selected_label = ['Air Conditioner']\n",
    "\n",
    "x_train, y_train, index_train = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=each_file_len,\n",
    "    feature_select=feature_select,\n",
    "    Transformer=load_transformer,\n",
    "    source='submetered_process2.1/training')  # 读取训练数据\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "x_validation, y_validation, index_validation = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=each_file_len,\n",
    "    feature_select=feature_select,\n",
    "    Transformer=load_transformer,\n",
    "    source='submetered_process2.1/validation')  # 读取验证数据\n",
    "y_validation = y_validation.reshape(-1, 1)\n",
    "\n",
    "x_trainval = np.concatenate((x_train, x_validation), axis=0)\n",
    "y_trainval = np.concatenate((y_train, y_validation), axis=0)\n",
    "\n",
    "x_test, y_test, index_test = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=each_file_len,\n",
    "    feature_select=feature_select,\n",
    "    Transformer=load_transformer,\n",
    "    source='submetered_process2.1/testing')  # 读取测试数据\n",
    "\n",
    "print('finished loading data, cost %.3fs' % (time.time() - start_reading_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished loading data, cost 13.893s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test cluster\n",
    "km=KMeans(n_clusters=10, random_state=1)  # 聚类中心为4个\n",
    "x=np.log(x_train[:,5]).reshape(-1, 1) # 取对数\n",
    "y_pred = km.fit_predict(x) # 进行聚类并输出每个样本的聚类编号\n",
    "t=range(0,x.shape[0],1) \n",
    "plt.scatter(t,x, c=y_pred)\n",
    "plt.show()\n",
    "label=km.labels_.tolist()\n",
    "print(Counter(label))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "从上述结果可以看到，使用传统的Kmeans聚类并没有很明显的稀疏分割边界，说明所有电器的特征过于连续，或者聚类方式有待改进。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "center_record = {}\n",
    "centers = {}\n",
    "feature_lens = 30  # 应该改成len函数取值\n",
    "for f_index in range(feature_lens):  #\n",
    "    # n_cluster = 4 * (2 * feature_lens - 2 * f_index) - 4 # 按照数圈的方式制定聚类中心\n",
    "    n_cluster = 10\n",
    "    km = KMeans(n_clusters=n_cluster, random_state=1)\n",
    "    if f_index in [5, 6]:  #只有这两个特征（功率）需要取对数\n",
    "        x = np.log(x_train[:, f_index]).reshape(-1, 1)\n",
    "    x = x_train[:, f_index].reshape(-1, 1)\n",
    "    y_pred = km.fit_predict(x).reshape(-1,\n",
    "                                       1)  # 这里的y只是kmean里面的聚类中心编号，不是从小到大排列下来的编号\n",
    "    y_train = np.concatenate((y_train, y_pred), axis=1)\n",
    "    centers[f_index] = [i for item in km.cluster_centers_\n",
    "                        for i in item]  # item是个数组，如果只有1维，再加个循环读取数值\n",
    "    center_record[f_index] = np.argsort(\n",
    "        centers[f_index]\n",
    "    )  # 对kmeans得到的聚类中心从小到大进行排序，得到排序index，即center[record[0]]为中心最小值\n",
    "y_label = y_train[:, 0]\n",
    "y_cluster = y_train[:, 1:]\n",
    "node_num_transform = {}\n",
    "node_label_transform = {}\n",
    "tmp = 1  # 好像公开数据集节点都是从1开始的\n",
    "for feat, cluster in product(range(feature_lens),\n",
    "                             range(n_cluster)):  #把所有特征的聚类值转化为顺序排列的整数\n",
    "    node_num_transform['%03d,%03d' % (feat, cluster)] = str(tmp) #转为用字符串，方便后续存储\n",
    "    node_label_transform[tmp] = np.array([feat, cluster\n",
    "                                          ])  # 暂时用不到拆分的，直接用tmp来代表特征和聚类中心\n",
    "    tmp += 1\n",
    "del tmp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 计算图特征并存储成数据\n",
    "\n",
    "\n",
    "# 输入点列表，返回所有边\n",
    "def calc_edge(Node_list):\n",
    "    edge_list = []\n",
    "    for node_A in Node_list:\n",
    "        for node_B in Node_list:\n",
    "            if node_B == node_A:\n",
    "                continue\n",
    "            edge_list.append(node_A + ',' + node_B)\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "# 记录边的重复出现次数\n",
    "def count_edge(edge_count_dict, edge_list):\n",
    "    for edge in edge_list:\n",
    "        if edge not in edge_count_dict.keys():\n",
    "            edge_count_dict[edge] = 1\n",
    "        else:\n",
    "            edge_count_dict[edge] += 1\n",
    "    return edge_count_dict  # key为边，格式为'%03d,%03d'；value为出现的次数\n",
    "\n",
    "\n",
    "# 记录点的重复出现次数\n",
    "def count_node(node_count_dict, node_list):  # node为字符串型\n",
    "    for node in node_list:\n",
    "        if node not in node_count_dict.keys():\n",
    "            node_count_dict[node] = 1\n",
    "        else:\n",
    "            node_count_dict[node] += 1\n",
    "    return node_count_dict\n",
    "\n",
    "\n",
    "def save_graph_in_txt(node_counts, edge_counts, graph_label) -> None:\n",
    "    file_A = open(\"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_A.txt\",\n",
    "                  'r+')\n",
    "    file_graph_indicator = open(\n",
    "        \"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_graph_indicator.txt\",\n",
    "        'r+')\n",
    "    file_graph_label = open(\n",
    "        \"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_graph_label.txt\",\n",
    "        'r+')\n",
    "    file_node_attributes = open(\n",
    "        \"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_node_attributes.txt\",\n",
    "        'r+')\n",
    "    file_node_labels = open(\n",
    "        \"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_node_labels.txt\",\n",
    "        'r+')\n",
    "    file_edge_attributes = open(\n",
    "        \"/home/chaofan/powerknowledge/graph/PLAIDG/PLAIDG_edge_attributes.txt\",\n",
    "        'r+')\n",
    "    index_convert = {}\n",
    "    base_node_num = len(file_node_labels.readlines())\n",
    "    base_graph_num = len(file_graph_label.readlines())\n",
    "    file_graph_label.write(str(graph_label) + '\\n')\n",
    "    tmp = 1\n",
    "    for key in node_counts.keys():  # key为节点类型，变成label；value为次数，变成attribute\n",
    "        file_node_labels.write(str(key) + '\\n')  #指示当前节点的标签（特征+中心所对应的编号）\n",
    "        file_node_attributes.write(str(node_counts[key]) + '\\n')  #指示当前节点的特征\n",
    "        file_graph_indicator.write(str(base_graph_num + 1) +\n",
    "                                   '\\n')  #指示当前节点属于第几个graph\n",
    "        index_convert[key] = str(base_node_num + tmp)  #用于描述A矩阵，重新定义边的节点编号\n",
    "        tmp += 1\n",
    "    for key in edge_counts.keys():  # key为节点对，变成A阵；value为次数，变成attribute\n",
    "        key_str = key.split(',')\n",
    "        node_a = index_convert[key_str[0]]\n",
    "        node_b = index_convert[key_str[1]]\n",
    "        file_A.write(node_a + ',' + node_b + '\\n')\n",
    "        file_edge_attributes.write(str(edge_counts[key]) + '\\n')\n",
    "    file_A.close()\n",
    "    file_graph_indicator.close()\n",
    "    file_graph_label.close()\n",
    "    file_node_attributes.close()\n",
    "    file_node_labels.close()\n",
    "    file_edge_attributes.close()\n",
    "\n",
    "\n",
    "file_count = 0\n",
    "for i, yi_cluster in enumerate(y_cluster):\n",
    "    # if i < 100:\n",
    "    #     continue  # 跳过100个数？\n",
    "    if i % each_file_len == 0:  #判断是否进入新的文件\n",
    "        edge_counts = {}\n",
    "        node_counts = {}\n",
    "        y_label_tmp = y_label[i]\n",
    "    nodes_temp = []\n",
    "\n",
    "    for j, cluster_k in enumerate(yi_cluster):  # j为特征编号,cluster_k为所属类别\n",
    "        cluster_k = int(cluster_k)\n",
    "        nodes_temp.append(node_num_transform[\"%03d,%03d\" %\n",
    "                                             (j, cluster_k)])  # 读取当前特征的节点编号\n",
    "\n",
    "    # calc edge\n",
    "    edge_list = calc_edge(nodes_temp)\n",
    "    edge_counts = count_edge(edge_counts, edge_list)\n",
    "    # calc node\n",
    "    node_counts = count_node(node_counts, nodes_temp)\n",
    "    if i % (each_file_len - 1) == 0:\n",
    "        save_graph_in_txt(node_counts, edge_counts, y_label_tmp)\n",
    "        print('%04d/%04d' % (file_count, int(len(y_label) / each_file_len)))\n",
    "        file_count += 1\n",
    "        break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0000/0792\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 画图用\n",
    "import networkx as nx\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# 计算每个特征中心的位置，输入为特征编号，特征值排序值\n",
    "def calc_pos(total_features_num, feature_index, sort_index):\n",
    "    edge_len = round(2 * (total_features_num - feature_index))\n",
    "    if sort_index < 0:\n",
    "        return None, None\n",
    "    elif sort_index < 0.5 * edge_len:\n",
    "        return round(0.5 * edge_len - 1), round(sort_index)\n",
    "    elif sort_index < 1.5 * edge_len - 1:\n",
    "        return round(edge_len - 2 - sort_index), round(0.5 * edge_len - 1)\n",
    "    elif sort_index < 2.5 * edge_len - 2:\n",
    "        return round(-0.5 * edge_len), round(2 * edge_len - 3 - sort_index)\n",
    "    elif sort_index < 3.5 * edge_len - 3:\n",
    "        return round(sort_index - 3 * edge_len + 3), round(-0.5 * edge_len)\n",
    "    elif sort_index < 4 * edge_len - 4:\n",
    "        return round(0.5 * edge_len - 1), round(sort_index - 4 * edge_len + 4)\n",
    "\n",
    "# 输入点列表，返回所有边\n",
    "def calc_edge(Node_list):  \n",
    "    edge_list = []\n",
    "    for node_A in Node_list:\n",
    "        for node_B in Node_list:\n",
    "            if node_B == node_A:\n",
    "                continue\n",
    "            edge_list.append((node_A, node_B))\n",
    "    return edge_list\n",
    "\n",
    "# 记录边的重复出现次数\n",
    "def count_edge(edge_count_dict, edge_list):  \n",
    "    for edge in edge_list:\n",
    "        if edge not in edge_count_dict.keys():\n",
    "            edge_count_dict[edge] = 1\n",
    "        else:\n",
    "            edge_count_dict[edge] += 1\n",
    "    return edge_count_dict\n",
    "\n",
    "# 记录点的重复出现次数\n",
    "def count_node(node_count_dict, node_list):\n",
    "    for node in node_list:\n",
    "        if node not in node_count_dict.keys():\n",
    "            node_count_dict[node] = 1\n",
    "        else:\n",
    "            node_count_dict[node] += 1 \n",
    "    return node_count_dict\n",
    "\n",
    "\n",
    "each_file_len = 10\n",
    "value_dict = {}\n",
    "for i, yi_cluster in enumerate(y_cluster):\n",
    "    if i < 100:\n",
    "        continue # 跳过100个数？\n",
    "    if i % each_file_len == 0:\n",
    "        edge_counts = {}\n",
    "        node_counts = {}\n",
    "        feature_len = len(yi_cluster)\n",
    "        Graph_i = nx.Graph(application=y_label[i])\n",
    "    nodes_temp = []\n",
    "\n",
    "    for j, cluster_j in enumerate(yi_cluster):   # j为特征编号\n",
    "        cluster_j = int(cluster_j)\n",
    "        cluster_index = np.argwhere(center_record[j] == cluster_j) # 找到当前特征聚类所处的排位\n",
    "        cluster_index = cluster_index[0][0]\n",
    "        x, y = calc_pos(feature_len, j, cluster_index)  # 计算在图的位置\n",
    "        # pos.append([x, y])\n",
    "        # Graph_i.add_node('%02d,%02d' % (x, y), value=centers[j][cluster_j])\n",
    "        # nodes.append('%02d,%02d' % (x, y))\n",
    "        nodes_temp.append('%02d,%02d' % (x, y))\n",
    "        value_dict['%02d,%02d' % (x, y)] = centers[j][cluster_j] # 把对应位置的真实特征值记录\n",
    "\n",
    "    # calc edge\n",
    "    edge_list = calc_edge(nodes_temp)\n",
    "    edge_counts = count_edge(edge_counts, edge_list)\n",
    "    # calc node\n",
    "    node_counts = count_node(node_counts, nodes_temp)\n",
    "\n",
    "    # draw nodes\n",
    "    if i % each_file_len == each_file_len - 1:\n",
    "        edge_select = []\n",
    "        node_select = []\n",
    "        pos = []\n",
    "        edge_threshold = 8 # 设置plot阈值\n",
    "        node_threshold = 8 # 设置plot阈值\n",
    "\n",
    "        for k, v in edge_counts.items():\n",
    "            if v >= edge_threshold:\n",
    "                edge_select.append(k)\n",
    "        Graph_i.add_edges_from(edge_select) # 添加边\n",
    "\n",
    "        for k, v in node_counts.items():\n",
    "            if v >= node_threshold:\n",
    "                node_select.append(k)\n",
    "                b = k.split(',')\n",
    "                pos.append([int(b[0]), int(b[1])])\n",
    "                Graph_i.add_node(k, value=value_dict[k])    # 添加点\n",
    "\n",
    "        figure1, ax1 = plt.subplots(1, 1) # 声明画布\n",
    "        npos = dict(zip(node_select, pos)) # 生成点组\n",
    "        nx.draw_networkx_nodes(Graph_i,\n",
    "                               npos,\n",
    "                               node_select,\n",
    "                               node_size=30,\n",
    "                               label=True)  # 画点\n",
    "        nx.draw_networkx_edges(\n",
    "            Graph_i,\n",
    "            npos,\n",
    "            edge_select,\n",
    "        )   # 画边\n",
    "        spacing = 0.5  # This can be your user specified spacing.\n",
    "        minorLocator = MultipleLocator(spacing)\n",
    "        # Set minor tick locations.\n",
    "        ax1.set_xticks(range(-feature_lens, feature_lens+1, 1))\n",
    "        ax1.set_yticks(range(-feature_lens, feature_lens+1, 1))\n",
    "        plt.xlim(-feature_lens-0.5, feature_lens+0.5)\n",
    "        plt.ylim(-feature_lens-0.5, feature_lens+0.5)\n",
    "        ax1.yaxis.set_minor_locator(minorLocator)\n",
    "        ax1.xaxis.set_minor_locator(minorLocator)\n",
    "        # Set grid to use minor tick locations.\n",
    "        ax1.grid(which='minor')\n",
    "        plt.show()\n",
    "        figure1.clear()\n",
    "        plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}