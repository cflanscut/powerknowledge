{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd0b5f9473948cb6cc6acefe1e4d7af67cc2b95399d24746c153a73ff8f3dd5bf7d",
   "display_name": "Python 3.8.2 64-bit ('anaconda3': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append('/home/chaofan/powerknowledge/data')\n",
    "# sys.path.append('data/')\n",
    "from read_PLAID_data import read_processed_data,get_feature_name\n",
    "\n",
    "def one_hot(labels,class_list):\n",
    "    labels_num=[]\n",
    "    for label in labels:\n",
    "        labels_num.append(class_list.index(label))\n",
    "    labels_num=torch.from_numpy(np.array(labels_num))\n",
    "    batch_size=len(labels)\n",
    "    class_num=len(class_list)\n",
    "    labels_num=labels_num.resize_(batch_size,1)\n",
    "    m_zeros=torch.zeros(batch_size,class_num)\n",
    "    onehot=m_zeros.scatter_(1,labels_num,1)\n",
    "    return onehot.numpy()\n",
    "\n",
    "def to_num(labels,class_list):\n",
    "    labels_num=[]\n",
    "    for label in labels:\n",
    "        labels_num.append(class_list.index(label))\n",
    "    return np.array(labels_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished loading data, cost 5.133s\n",
      "Accuracy : 0.9889\n",
      "Accuracy : 0.7995\n",
      "AUC Score (test): 0.981166\n"
     ]
    }
   ],
   "source": [
    "# test normal appliance\n",
    "start_reading_time = time.time()\n",
    "feature_select=get_feature_name('/home/chaofan/powerknowledge/data/source/submetered_zengj/total')\n",
    "selected_label = [\n",
    "    'Air Conditioner', 'Coffee maker', 'Fan', 'Fridge', 'Hair Iron',\n",
    "    'Hairdryer', 'Heater', 'Incandescent Light Bulb', 'Microwave',\n",
    "    'Soldering Iron', 'Vacuum', 'Washing Machine', 'Water kettle'\n",
    "]\n",
    "x_normal_train, y_normal_train, index_normal_train = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/training')\n",
    "\n",
    "x_normal_validation, y_normal_validation, index_normal_validation = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/validation')\n",
    "\n",
    "x_normal_trainval = np.concatenate((x_normal_train, x_normal_validation), axis=0)\n",
    "y_normal_trainval = np.concatenate((y_normal_train, y_normal_validation), axis=0)\n",
    "\n",
    "x_normal_test, y_normal_test, index_normal_test = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/testing')\n",
    "\n",
    "y_normal_train_onehot=one_hot(y_normal_train,selected_label)\n",
    "y_normal_validation_onehot=one_hot(y_normal_validation,selected_label)\n",
    "y_normal_trainval_onehot=one_hot(y_normal_trainval,selected_label)\n",
    "y_normal_test_onehot=one_hot(y_normal_test,selected_label)\n",
    "\n",
    "y_normal_train_num=to_num(y_normal_train,selected_label)\n",
    "y_normal_validation_num=to_num(y_normal_validation,selected_label)\n",
    "y_normal_trainval_num=to_num(y_normal_trainval,selected_label)\n",
    "y_normal_test_num=to_num(y_normal_test,selected_label)\n",
    "\n",
    "print('finished loading data, cost %.3fs' % (time.time() - start_reading_time))\n",
    "\n",
    "normal_rf = RandomForestClassifier(n_estimators=200,\n",
    "                                min_samples_split=10,\n",
    "                                min_samples_leaf=5,\n",
    "                                max_depth=8,\n",
    "                                max_features='auto',\n",
    "                                random_state=10)\n",
    "normal_rf.fit(x_normal_trainval, y_normal_trainval)\n",
    "\n",
    "y_normal_trainval_pred = normal_rf.predict(x_normal_trainval)\n",
    "y_normal_trainval_predprob = normal_rf.predict_proba(x_normal_trainval)\n",
    "\n",
    "y_normal_test_pred = normal_rf.predict(x_normal_test)\n",
    "y_normal_test_predprob = normal_rf.predict_proba(x_normal_test)\n",
    "\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_normal_trainval, y_normal_trainval_pred))\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_normal_test, y_normal_test_pred))\n",
    "\n",
    "print(\"AUC Score (test): %f\" %\n",
    "      metrics.roc_auc_score(y_normal_test_onehot, y_normal_test_predprob, average='micro'))\n",
    "\n",
    "y_normal_trainval_predprob=torch.from_numpy(y_normal_trainval_predprob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc_train:0.9899,loss:0.0605,acc_test:0.6900\n",
      "acc_train:0.9717,loss:0.1001,acc_test:0.6224\n",
      "acc_train:0.9575,loss:0.1124,acc_test:0.5664\n",
      "acc_train:0.9757,loss:0.0967,acc_test:0.6573\n",
      "acc_train:0.9717,loss:0.1162,acc_test:0.6946\n",
      "acc_train:0.9706,loss:0.1049,acc_test:0.6084\n",
      "acc_train:0.9949,loss:0.0548,acc_test:0.6597\n",
      "acc_train:0.9848,loss:0.0899,acc_test:0.6294\n",
      "acc_train:0.9464,loss:0.1686,acc_test:0.5828\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim,hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim,hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim,hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim,hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_normal_trainval_tensor=torch.from_numpy(x_normal_trainval).to(torch.float32)\n",
    "y_normal_trainval_onehot_tensor=torch.from_numpy(y_normal_trainval_onehot).to(torch.float32)\n",
    "y_normal_trainval_tensor=torch.from_numpy(y_normal_trainval_num)\n",
    "\n",
    "x_normal_test_tensor=torch.from_numpy(x_normal_test).to(torch.float32)\n",
    "y_normal_test_onehot_tensor=torch.from_numpy(y_normal_test_onehot).to(torch.float32)\n",
    "\n",
    "for times in range(1,10,1):\n",
    "    nn_clf=nn_student(34,32,14)\n",
    "    optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.1)\n",
    "    loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    for i in range(10000):\n",
    "        acc_train=0.0\n",
    "        acc_test=0.0\n",
    "        prediction=nn_clf(x_normal_trainval_tensor)\n",
    "        loss=loss_fn(prediction,y_normal_trainval_tensor)  # crossentroy 对于多分类直接用数字\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%9999==0 and i!=0:\n",
    "            acc_train += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "                    np.argmax(y_normal_trainval_onehot,axis=1))\n",
    "            x_normal_test_pred = nn_clf(x_normal_test_tensor)\n",
    "            acc_test += np.sum(np.argmax(x_normal_test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                            np.argmax(y_normal_test_onehot,axis=1))\n",
    "            print('acc_train:%.4f,loss:%.4f,acc_test:%.4f'%(acc_train/len(y_normal_trainval_onehot),loss.item(),acc_test/len(y_normal_test_onehot)))\n",
    "    # test_pred=nn_clf(x_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-19c8d18cc57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_student\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_teacher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 找最佳T和a\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_normal_trainval_tensor=torch.from_numpy(x_normal_trainval).to(torch.float32)\n",
    "y_normal_trainval_onehot_tensor=torch.from_numpy(y_normal_trainval_onehot).to(torch.float32)\n",
    "y_normal_trainval_tensor=torch.from_numpy(y_normal_trainval_num)\n",
    "\n",
    "\n",
    "# alpha=0.8\n",
    "# T=2\n",
    "best_score=0.0\n",
    "best_alpha=0\n",
    "best_T=0\n",
    "for alpha in [0.05,0.1,0.2,0.25,0.5,0.75,0.8,0.9,0.95,1]:\n",
    "    for T in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        nn_clf=nn_student(34,32,13)\n",
    "        optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.01)\n",
    "        loss_fn1=torch.nn.CrossEntropyLoss()\n",
    "        loss_fn2=torch.nn.KLDivLoss()\n",
    "        for i in range(100000):\n",
    "            acc=0.0\n",
    "            prediction=nn_clf(x_normal_trainval_tensor)\n",
    "            loss1=loss_fn1(prediction,y_normal_trainval_tensor)\n",
    "            \n",
    "            output_student=F.log_softmax(prediction/T,dim=1).to(torch.float32)\n",
    "            # output_teacher=F.softmax(y_trainval_predprob/T,dim=1).to(torch.float32)\n",
    "            output_teacher=y_normal_trainval_predprob.to(torch.float32)\n",
    "            loss2=loss_fn2(output_student,output_teacher)*T*T\n",
    "            optimizer.zero_grad()\n",
    "            loss=loss1*(1-alpha)+loss2*alpha\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if i%1000==0:\n",
    "            #     acc += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "            #             np.argmax(y_trainval_onehot,axis=1))\n",
    "            #     print('acc:%.5f'%(acc/len(y_trainval_onehot)))\n",
    "            #     print('[%d, %5d] loss: %.4f loss1: %.4f loss2: %.4f' %(i, 100000, loss.item(), loss1.item(), loss2.item()))\n",
    "\n",
    "        x_normal_test_tensor=torch.from_numpy(x_normal_test).to(torch.float32)\n",
    "        y_normal_test_onehot_tensor=torch.from_numpy(y_normal_test_onehot).to(torch.float32)\n",
    "        test_pred=nn_clf(x_normal_test_tensor)\n",
    "        acc=0.0\n",
    "        acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                        np.argmax(y_normal_test_onehot,axis=1))\n",
    "        acc=acc/len(y_normal_test_onehot)\n",
    "        if acc>best_score:\n",
    "            best_score=acc\n",
    "            best_alpha=alpha\n",
    "            best_T=T\n",
    "        print('alpha:%.2f,T:%02d,acc:%.5f'%(alpha,T,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc:0.10732\n",
      "[0, 100000] loss: 1.4579 loss1: 2.7084 loss2: 1.3189\n",
      "acc:0.45135\n",
      "[1000, 100000] loss: 1.0761 loss1: 1.7544 loss2: 1.0008\n",
      "acc:0.49549\n",
      "[2000, 100000] loss: 0.9243 loss1: 1.5529 loss2: 0.8544\n",
      "acc:0.53360\n",
      "[3000, 100000] loss: 0.8311 loss1: 1.4437 loss2: 0.7630\n",
      "acc:0.54463\n",
      "[4000, 100000] loss: 0.7625 loss1: 1.3570 loss2: 0.6965\n",
      "acc:0.55567\n",
      "[5000, 100000] loss: 0.7091 loss1: 1.2916 loss2: 0.6443\n",
      "acc:0.58576\n",
      "[6000, 100000] loss: 0.6662 loss1: 1.2404 loss2: 0.6024\n",
      "acc:0.59378\n",
      "[7000, 100000] loss: 0.6315 loss1: 1.1996 loss2: 0.5684\n",
      "acc:0.61384\n",
      "[8000, 100000] loss: 0.6028 loss1: 1.1694 loss2: 0.5399\n",
      "acc:0.64193\n",
      "[9000, 100000] loss: 0.5789 loss1: 1.1431 loss2: 0.5163\n",
      "acc:0.65697\n",
      "[10000, 100000] loss: 0.5573 loss1: 1.1094 loss2: 0.4960\n",
      "acc:0.67101\n",
      "[11000, 100000] loss: 0.5378 loss1: 1.0843 loss2: 0.4771\n",
      "acc:0.67904\n",
      "[12000, 100000] loss: 0.5184 loss1: 1.0519 loss2: 0.4591\n",
      "acc:0.68205\n",
      "[13000, 100000] loss: 0.4990 loss1: 1.0122 loss2: 0.4420\n",
      "acc:0.70010\n",
      "[14000, 100000] loss: 0.4816 loss1: 0.9777 loss2: 0.4264\n",
      "acc:0.70812\n",
      "[15000, 100000] loss: 0.4650 loss1: 0.9489 loss2: 0.4112\n",
      "acc:0.70913\n",
      "[16000, 100000] loss: 0.4492 loss1: 0.9237 loss2: 0.3965\n",
      "acc:0.72317\n",
      "[17000, 100000] loss: 0.4385 loss1: 0.9046 loss2: 0.3867\n",
      "acc:0.72417\n",
      "[18000, 100000] loss: 0.4236 loss1: 0.8809 loss2: 0.3728\n",
      "acc:0.73220\n",
      "[19000, 100000] loss: 0.4139 loss1: 0.8633 loss2: 0.3640\n",
      "acc:0.73320\n",
      "[20000, 100000] loss: 0.4014 loss1: 0.8441 loss2: 0.3523\n",
      "acc:0.73621\n",
      "[21000, 100000] loss: 0.3923 loss1: 0.8305 loss2: 0.3436\n",
      "acc:0.73521\n",
      "[22000, 100000] loss: 0.3813 loss1: 0.8105 loss2: 0.3336\n",
      "acc:0.74022\n",
      "[23000, 100000] loss: 0.3723 loss1: 0.7930 loss2: 0.3256\n",
      "acc:0.74122\n",
      "[24000, 100000] loss: 0.3645 loss1: 0.7817 loss2: 0.3182\n",
      "acc:0.74624\n",
      "[25000, 100000] loss: 0.3565 loss1: 0.7674 loss2: 0.3108\n",
      "acc:0.74624\n",
      "[26000, 100000] loss: 0.3490 loss1: 0.7528 loss2: 0.3042\n",
      "acc:0.74524\n",
      "[27000, 100000] loss: 0.3424 loss1: 0.7427 loss2: 0.2979\n",
      "acc:0.74524\n",
      "[28000, 100000] loss: 0.3358 loss1: 0.7306 loss2: 0.2920\n",
      "acc:0.74624\n",
      "[29000, 100000] loss: 0.3299 loss1: 0.7205 loss2: 0.2865\n",
      "acc:0.76229\n",
      "[30000, 100000] loss: 0.3243 loss1: 0.7114 loss2: 0.2813\n",
      "acc:0.77232\n",
      "[31000, 100000] loss: 0.3186 loss1: 0.7023 loss2: 0.2760\n",
      "acc:0.77633\n",
      "[32000, 100000] loss: 0.3135 loss1: 0.6925 loss2: 0.2713\n",
      "acc:0.78134\n",
      "[33000, 100000] loss: 0.3074 loss1: 0.6775 loss2: 0.2663\n",
      "acc:0.78335\n",
      "[34000, 100000] loss: 0.3008 loss1: 0.6585 loss2: 0.2611\n",
      "acc:0.78736\n",
      "[35000, 100000] loss: 0.2949 loss1: 0.6443 loss2: 0.2560\n",
      "acc:0.79539\n",
      "[36000, 100000] loss: 0.2903 loss1: 0.6361 loss2: 0.2519\n",
      "acc:0.79840\n",
      "[37000, 100000] loss: 0.2860 loss1: 0.6268 loss2: 0.2481\n",
      "acc:0.80040\n",
      "[38000, 100000] loss: 0.2810 loss1: 0.6137 loss2: 0.2440\n",
      "acc:0.80241\n",
      "[39000, 100000] loss: 0.2770 loss1: 0.6059 loss2: 0.2405\n",
      "acc:0.80341\n",
      "[40000, 100000] loss: 0.2733 loss1: 0.5985 loss2: 0.2371\n",
      "acc:0.80642\n",
      "[41000, 100000] loss: 0.2696 loss1: 0.5913 loss2: 0.2339\n",
      "acc:0.80742\n",
      "[42000, 100000] loss: 0.2661 loss1: 0.5843 loss2: 0.2308\n",
      "acc:0.80742\n",
      "[43000, 100000] loss: 0.2628 loss1: 0.5776 loss2: 0.2278\n",
      "acc:0.80742\n",
      "[44000, 100000] loss: 0.2595 loss1: 0.5706 loss2: 0.2249\n",
      "acc:0.80642\n",
      "[45000, 100000] loss: 0.2563 loss1: 0.5642 loss2: 0.2221\n",
      "acc:0.80642\n",
      "[46000, 100000] loss: 0.2533 loss1: 0.5578 loss2: 0.2195\n",
      "acc:0.80742\n",
      "[47000, 100000] loss: 0.2504 loss1: 0.5515 loss2: 0.2169\n",
      "acc:0.81846\n",
      "[48000, 100000] loss: 0.2475 loss1: 0.5453 loss2: 0.2145\n",
      "acc:0.83551\n",
      "[49000, 100000] loss: 0.2447 loss1: 0.5388 loss2: 0.2120\n",
      "acc:0.83952\n",
      "[50000, 100000] loss: 0.2420 loss1: 0.5331 loss2: 0.2097\n",
      "acc:0.84253\n",
      "[51000, 100000] loss: 0.2393 loss1: 0.5263 loss2: 0.2074\n",
      "acc:0.84554\n",
      "[52000, 100000] loss: 0.2368 loss1: 0.5207 loss2: 0.2052\n",
      "acc:0.84855\n",
      "[53000, 100000] loss: 0.2343 loss1: 0.5147 loss2: 0.2031\n",
      "acc:0.84955\n",
      "[54000, 100000] loss: 0.2319 loss1: 0.5096 loss2: 0.2011\n",
      "acc:0.85155\n",
      "[55000, 100000] loss: 0.2297 loss1: 0.5047 loss2: 0.1991\n",
      "acc:0.85055\n",
      "[56000, 100000] loss: 0.2275 loss1: 0.5000 loss2: 0.1972\n",
      "acc:0.85256\n",
      "[57000, 100000] loss: 0.2251 loss1: 0.4945 loss2: 0.1951\n",
      "acc:0.85557\n",
      "[58000, 100000] loss: 0.2230 loss1: 0.4892 loss2: 0.1934\n",
      "acc:0.85657\n",
      "[59000, 100000] loss: 0.2209 loss1: 0.4845 loss2: 0.1916\n",
      "acc:0.85657\n",
      "[60000, 100000] loss: 0.2189 loss1: 0.4800 loss2: 0.1899\n",
      "acc:0.85858\n",
      "[61000, 100000] loss: 0.2169 loss1: 0.4754 loss2: 0.1882\n",
      "acc:0.85858\n",
      "[62000, 100000] loss: 0.2145 loss1: 0.4657 loss2: 0.1866\n",
      "acc:0.85757\n",
      "[63000, 100000] loss: 0.2121 loss1: 0.4617 loss2: 0.1844\n",
      "acc:0.85657\n",
      "[64000, 100000] loss: 0.2100 loss1: 0.4570 loss2: 0.1826\n",
      "acc:0.85657\n",
      "[65000, 100000] loss: 0.2081 loss1: 0.4524 loss2: 0.1809\n",
      "acc:0.86158\n",
      "[66000, 100000] loss: 0.2061 loss1: 0.4470 loss2: 0.1793\n",
      "acc:0.86158\n",
      "[67000, 100000] loss: 0.2042 loss1: 0.4427 loss2: 0.1777\n",
      "acc:0.86158\n",
      "[68000, 100000] loss: 0.2025 loss1: 0.4385 loss2: 0.1762\n",
      "acc:0.86158\n",
      "[69000, 100000] loss: 0.2007 loss1: 0.4344 loss2: 0.1747\n",
      "acc:0.86158\n",
      "[70000, 100000] loss: 0.1989 loss1: 0.4306 loss2: 0.1732\n",
      "acc:0.86158\n",
      "[71000, 100000] loss: 0.1972 loss1: 0.4268 loss2: 0.1717\n",
      "acc:0.86158\n",
      "[72000, 100000] loss: 0.1956 loss1: 0.4231 loss2: 0.1703\n",
      "acc:0.86158\n",
      "[73000, 100000] loss: 0.1941 loss1: 0.4195 loss2: 0.1690\n",
      "acc:0.86158\n",
      "[74000, 100000] loss: 0.1926 loss1: 0.4161 loss2: 0.1677\n",
      "acc:0.86158\n",
      "[75000, 100000] loss: 0.1911 loss1: 0.4128 loss2: 0.1665\n",
      "acc:0.86359\n",
      "[76000, 100000] loss: 0.1897 loss1: 0.4096 loss2: 0.1653\n",
      "acc:0.86359\n",
      "[77000, 100000] loss: 0.1883 loss1: 0.4066 loss2: 0.1641\n",
      "acc:0.86359\n",
      "[78000, 100000] loss: 0.1870 loss1: 0.4036 loss2: 0.1630\n",
      "acc:0.86359\n",
      "[79000, 100000] loss: 0.1857 loss1: 0.4001 loss2: 0.1619\n",
      "acc:0.86760\n",
      "[80000, 100000] loss: 0.1841 loss1: 0.3953 loss2: 0.1607\n",
      "acc:0.86760\n",
      "[81000, 100000] loss: 0.1828 loss1: 0.3922 loss2: 0.1595\n",
      "acc:0.86760\n",
      "[82000, 100000] loss: 0.1815 loss1: 0.3895 loss2: 0.1584\n",
      "acc:0.87161\n",
      "[83000, 100000] loss: 0.1802 loss1: 0.3866 loss2: 0.1572\n",
      "acc:0.87262\n",
      "[84000, 100000] loss: 0.1789 loss1: 0.3840 loss2: 0.1561\n",
      "acc:0.87061\n",
      "[85000, 100000] loss: 0.1777 loss1: 0.3818 loss2: 0.1550\n",
      "acc:0.87663\n",
      "[86000, 100000] loss: 0.1765 loss1: 0.3794 loss2: 0.1539\n",
      "acc:0.85456\n",
      "[87000, 100000] loss: 0.1903 loss1: 0.4216 loss2: 0.1645\n",
      "acc:0.87763\n",
      "[88000, 100000] loss: 0.1734 loss1: 0.3689 loss2: 0.1517\n",
      "acc:0.88265\n",
      "[89000, 100000] loss: 0.1676 loss1: 0.3462 loss2: 0.1478\n",
      "acc:0.89769\n",
      "[90000, 100000] loss: 0.1659 loss1: 0.3436 loss2: 0.1461\n",
      "acc:0.89769\n",
      "[91000, 100000] loss: 0.1646 loss1: 0.3413 loss2: 0.1450\n",
      "acc:0.89870\n",
      "[92000, 100000] loss: 0.1633 loss1: 0.3383 loss2: 0.1439\n",
      "acc:0.89970\n",
      "[93000, 100000] loss: 0.1611 loss1: 0.3332 loss2: 0.1420\n",
      "acc:0.89970\n",
      "[94000, 100000] loss: 0.1599 loss1: 0.3305 loss2: 0.1409\n",
      "acc:0.89970\n",
      "[95000, 100000] loss: 0.1590 loss1: 0.3283 loss2: 0.1402\n",
      "acc:0.89970\n",
      "[96000, 100000] loss: 0.1579 loss1: 0.3264 loss2: 0.1392\n",
      "acc:0.90070\n",
      "[97000, 100000] loss: 0.1566 loss1: 0.3225 loss2: 0.1382\n",
      "acc:0.90572\n",
      "[98000, 100000] loss: 0.1540 loss1: 0.3133 loss2: 0.1363\n",
      "acc:0.90572\n",
      "[99000, 100000] loss: 0.1526 loss1: 0.3106 loss2: 0.1350\n",
      "alpha:0.90,T:03,acc:0.66975\n"
     ]
    }
   ],
   "source": [
    "# 查看最佳参数下收敛情况\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_trainval_tensor=torch.from_numpy(x_trainval).to(torch.float32)\n",
    "y_trainval_onehot_tensor=torch.from_numpy(y_trainval_onehot).to(torch.float32)\n",
    "y_trainval_tensor=torch.from_numpy(y_trainval_num)\n",
    "\n",
    "\n",
    "alpha=0.9\n",
    "T=3\n",
    "\n",
    "nn_clf=nn_student(34,32,13)\n",
    "optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.01)\n",
    "loss_fn1=torch.nn.CrossEntropyLoss()\n",
    "loss_fn2=torch.nn.KLDivLoss()\n",
    "for i in range(100000):\n",
    "    acc=0.0\n",
    "    prediction=nn_clf(x_trainval_tensor)\n",
    "    loss1=loss_fn1(prediction,y_trainval_tensor)\n",
    "    \n",
    "    output_student=F.log_softmax(prediction/T,dim=1).to(torch.float32)\n",
    "    # output_teacher=F.softmax(y_trainval_predprob/T,dim=1).to(torch.float32)\n",
    "    output_teacher=y_trainval_predprob.to(torch.float32)\n",
    "    loss2=loss_fn2(output_student,output_teacher)*T*T\n",
    "    optimizer.zero_grad()\n",
    "    loss=loss1*(1-alpha)+loss2*alpha\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        acc += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "                np.argmax(y_trainval_onehot,axis=1))\n",
    "        print('acc:%.5f'%(acc/len(y_trainval_onehot)))\n",
    "        print('[%d, %5d] loss: %.4f loss1: %.4f loss2: %.4f' %(i, 100000, loss.item(), loss1.item(), loss2.item()))\n",
    "\n",
    "x_test_tensor=torch.from_numpy(x_test).to(torch.float32)\n",
    "y_test_onehot_tensor=torch.from_numpy(y_test_onehot).to(torch.float32)\n",
    "test_pred=nn_clf(x_test_tensor)\n",
    "acc=0.0\n",
    "acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                np.argmax(y_test_onehot,axis=1))\n",
    "acc=acc/len(y_test_onehot)\n",
    "\n",
    "print('alpha:%.2f,T:%02d,acc:%.5f'%(alpha,T,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished loading data, cost 21.120s\n"
     ]
    }
   ],
   "source": [
    "# total data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_reading_time = time.time()\n",
    "feature_select=get_feature_name('/home/chaofan/powerknowledge/data/source/submetered_zengj/total')\n",
    "\n",
    "x, y, index = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=10,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/total')\n",
    "\n",
    "print('finished loading data, cost %.3fs' % (time.time() - start_reading_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}