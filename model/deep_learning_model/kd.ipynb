{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd0b5f9473948cb6cc6acefe1e4d7af67cc2b95399d24746c153a73ff8f3dd5bf7d",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append('/home/chaofan/powerknowledge/data')\n",
    "# sys.path.append('data/')\n",
    "from read_PLAID_data import read_processed_data,get_feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished loading data, cost 9.145s\n"
     ]
    }
   ],
   "source": [
    "# test zengj\n",
    "def one_hot(labels,class_list):\n",
    "    labels_num=[]\n",
    "    for label in labels:\n",
    "        labels_num.append(class_list.index(label))\n",
    "    labels_num=torch.from_numpy(np.array(labels_num))\n",
    "    batch_size=len(labels)\n",
    "    class_num=len(class_list)\n",
    "    labels_num=labels_num.resize_(batch_size,1)\n",
    "    m_zeros=torch.zeros(batch_size,class_num)\n",
    "    onehot=m_zeros.scatter_(1,labels_num,1)\n",
    "    return onehot.numpy()\n",
    "\n",
    "def to_num(labels,class_list):\n",
    "    labels_num=[]\n",
    "    for label in labels:\n",
    "        labels_num.append(class_list.index(label))\n",
    "    return np.array(labels_num)\n",
    "\n",
    "start_reading_time = time.time()\n",
    "feature_select=get_feature_name('/home/chaofan/powerknowledge/data/source/submetered_zengj/total')\n",
    "selected_label = [\n",
    "    'Air Conditioner', 'Blender', 'Coffee maker', 'Fan', 'Fridge', 'Hair Iron',\n",
    "    'Hairdryer', 'Heater', 'Incandescent Light Bulb', 'Microwave',\n",
    "    'Soldering Iron', 'Vacuum', 'Washing Machine', 'Water kettle'\n",
    "]\n",
    "x_train, y_train, index_train = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/training')\n",
    "\n",
    "x_validation, y_validation, index_validation = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/validation')\n",
    "\n",
    "x_trainval = np.concatenate((x_train, x_validation), axis=0)\n",
    "y_trainval = np.concatenate((y_train, y_validation), axis=0)\n",
    "\n",
    "x_test, y_test, index_test = read_processed_data(\n",
    "    'type',\n",
    "    type_header='appliance',\n",
    "    selected_label=selected_label,\n",
    "    direaction=1,\n",
    "    offset=0,\n",
    "    each_lenth=1,\n",
    "    feature_select=feature_select,\n",
    "    source='submetered_zengj/testing')\n",
    "\n",
    "y_train_onehot=one_hot(y_train,selected_label)\n",
    "y_validation_onehot=one_hot(y_validation,selected_label)\n",
    "y_trainval_onehot=one_hot(y_trainval,selected_label)\n",
    "y_test_onehot=one_hot(y_test,selected_label)\n",
    "\n",
    "y_train_num=to_num(y_train,selected_label)\n",
    "y_validation_num=to_num(y_validation,selected_label)\n",
    "y_trainval_num=to_num(y_trainval,selected_label)\n",
    "y_test_num=to_num(y_test,selected_label)\n",
    "\n",
    "print('finished loading data, cost %.3fs' % (time.time() - start_reading_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 0.986\nAccuracy : 0.8083\nAUC Score (test): 0.979803\n"
     ]
    }
   ],
   "source": [
    "gbc0 = RandomForestClassifier(n_estimators=200,\n",
    "                                min_samples_split=10,\n",
    "                                min_samples_leaf=5,\n",
    "                                max_depth=8,\n",
    "                                max_features='auto',\n",
    "                                random_state=10)\n",
    "gbc0.fit(x_trainval, y_trainval)\n",
    "y_trainval_pred = gbc0.predict(x_trainval)\n",
    "y_trainval_predprob = gbc0.predict_proba(x_trainval)\n",
    "\n",
    "y_test_pred = gbc0.predict(x_test)\n",
    "y_test_predprob = gbc0.predict_proba(x_test)\n",
    "# y_test_one_hot = LabelBinarizer().fit_transform(y_test)\n",
    "\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_trainval, y_trainval_pred))\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "print(\"AUC Score (test): %f\" %\n",
    "      metrics.roc_auc_score(y_test_onehot, y_test_predprob, average='micro'))\n",
    "y_trainval_predprob=torch.from_numpy(y_trainval_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class nn_classifier(nn.Module):\n",
    "#     def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "#         super(nn_classifier,self).__init__()\n",
    "#         self.fc=nn.Sequential(\n",
    "#             nn.Linear(input_dim,hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim,hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim,hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim,hidden_dim),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(hidden_dim,output_dim),\n",
    "#             nn.Softmax()\n",
    "#         )\n",
    "#     def forward(self,x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# x_trainval_tensor=torch.from_numpy(x_trainval).to(torch.float32)\n",
    "# y_trainval_onehot_tensor=torch.from_numpy(y_trainval_onehot).to(torch.float32)\n",
    "\n",
    "# nn_clf=nn_classifier(34,32,14)\n",
    "# optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.3)\n",
    "# loss_fn=torch.nn.MSELoss()\n",
    "# for i in range(30000):\n",
    "#     acc=0.0\n",
    "#     prediction=nn_clf(x_trainval_tensor)\n",
    "#     loss=loss_fn(prediction,y_trainval_onehot_tensor)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if i%1000==0:\n",
    "#         acc += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "#                 np.argmax(y_trainval_onehot,axis=1))\n",
    "#         print('acc:%.5f'%(acc/len(y_trainval_onehot)))\n",
    "\n",
    "# x_test_tensor=torch.from_numpy(x_test).to(torch.float32)\n",
    "# y_test_onehot_tensor=torch.from_numpy(y_test_onehot).to(torch.float32)\n",
    "# test_pred=nn_clf(x_test_tensor)\n",
    "# acc=0.0\n",
    "# acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "#                 np.argmax(y_test_onehot,axis=1))\n",
    "# print('acc:%.5f'%(acc/len(y_test_onehot)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc_train:0.9970,loss:0.0200,acc_test:0.6282\n",
      "acc_train:0.9900,loss:0.0427,acc_test:0.6420\n",
      "acc_train:0.9930,loss:0.0390,acc_test:0.6236\n",
      "acc_train:0.9970,loss:0.0168,acc_test:0.5751\n",
      "acc_train:0.9980,loss:0.0166,acc_test:0.5935\n",
      "acc_train:0.9809,loss:0.0624,acc_test:0.6259\n",
      "acc_train:0.9980,loss:0.0186,acc_test:0.5566\n",
      "acc_train:0.9930,loss:0.0249,acc_test:0.5797\n",
      "acc_train:0.9920,loss:0.0452,acc_test:0.5958\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_trainval_tensor=torch.from_numpy(x_trainval).to(torch.float32)\n",
    "y_trainval_onehot_tensor=torch.from_numpy(y_trainval_onehot).to(torch.float32)\n",
    "y_trainval_tensor=torch.from_numpy(y_trainval_num)\n",
    "\n",
    "x_test_tensor=torch.from_numpy(x_test).to(torch.float32)\n",
    "y_test_onehot_tensor=torch.from_numpy(y_test_onehot).to(torch.float32)\n",
    "\n",
    "for times in range(1,10,1):\n",
    "    nn_clf=nn_student(34,32,14)\n",
    "    optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.1)\n",
    "    loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    for i in range(10000):\n",
    "        acc_train=0.0\n",
    "        acc_test=0.0\n",
    "        prediction=nn_clf(x_trainval_tensor)\n",
    "        loss=loss_fn(prediction,y_trainval_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%9999==0 and i!=0:\n",
    "            acc_train += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "                    np.argmax(y_trainval_onehot,axis=1))\n",
    "            test_pred=nn_clf(x_test_tensor)\n",
    "            acc_test += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                            np.argmax(y_test_onehot,axis=1))\n",
    "            print('acc_train:%.4f,loss:%.4f,acc_test:%.4f'%(acc_train/len(y_trainval_onehot),loss.item(),acc_test/len(y_test_onehot)))\n",
    "    # test_pred=nn_clf(x_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "alpha:0.05,T:01,acc:0.62125\n",
      "alpha:0.05,T:02,acc:0.57275\n",
      "alpha:0.05,T:03,acc:0.61894\n",
      "alpha:0.05,T:04,acc:0.63972\n",
      "alpha:0.05,T:05,acc:0.68360\n",
      "alpha:0.05,T:06,acc:0.57506\n",
      "alpha:0.05,T:07,acc:0.59815\n",
      "alpha:0.05,T:08,acc:0.57275\n",
      "alpha:0.05,T:09,acc:0.60970\n",
      "alpha:0.05,T:10,acc:0.61201\n",
      "alpha:0.10,T:01,acc:0.59353\n",
      "alpha:0.10,T:02,acc:0.56813\n",
      "alpha:0.10,T:03,acc:0.60046\n",
      "alpha:0.10,T:04,acc:0.68360\n",
      "alpha:0.10,T:05,acc:0.57737\n",
      "alpha:0.10,T:06,acc:0.64203\n",
      "alpha:0.10,T:07,acc:0.59584\n",
      "alpha:0.10,T:08,acc:0.61201\n",
      "alpha:0.10,T:09,acc:0.59815\n",
      "alpha:0.10,T:10,acc:0.59584\n",
      "alpha:0.20,T:01,acc:0.63741\n",
      "alpha:0.20,T:02,acc:0.67898\n",
      "alpha:0.20,T:03,acc:0.59353\n",
      "alpha:0.20,T:04,acc:0.64896\n",
      "alpha:0.20,T:05,acc:0.62587\n",
      "alpha:0.20,T:06,acc:0.63048\n",
      "alpha:0.20,T:07,acc:0.62125\n",
      "alpha:0.20,T:08,acc:0.62125\n",
      "alpha:0.20,T:09,acc:0.61432\n",
      "alpha:0.20,T:10,acc:0.65589\n",
      "alpha:0.25,T:01,acc:0.64203\n",
      "alpha:0.25,T:02,acc:0.64203\n",
      "alpha:0.25,T:03,acc:0.55427\n",
      "alpha:0.25,T:04,acc:0.62818\n",
      "alpha:0.25,T:05,acc:0.60739\n",
      "alpha:0.25,T:06,acc:0.64434\n",
      "alpha:0.25,T:07,acc:0.65127\n",
      "alpha:0.25,T:08,acc:0.62818\n",
      "alpha:0.25,T:09,acc:0.65358\n",
      "alpha:0.25,T:10,acc:0.60739\n",
      "alpha:0.50,T:01,acc:0.62125\n",
      "alpha:0.50,T:02,acc:0.57044\n",
      "alpha:0.50,T:03,acc:0.63741\n",
      "alpha:0.50,T:04,acc:0.64896\n",
      "alpha:0.50,T:05,acc:0.62818\n",
      "alpha:0.50,T:06,acc:0.62125\n",
      "alpha:0.50,T:07,acc:0.62125\n",
      "alpha:0.50,T:08,acc:0.65127\n",
      "alpha:0.50,T:09,acc:0.62356\n",
      "alpha:0.50,T:10,acc:0.63510\n",
      "alpha:0.75,T:01,acc:0.70670\n",
      "alpha:0.75,T:02,acc:0.61432\n",
      "alpha:0.75,T:03,acc:0.61201\n",
      "alpha:0.75,T:04,acc:0.68822\n",
      "alpha:0.75,T:05,acc:0.61894\n",
      "alpha:0.75,T:06,acc:0.61201\n",
      "alpha:0.75,T:07,acc:0.62587\n",
      "alpha:0.75,T:08,acc:0.59815\n",
      "alpha:0.75,T:09,acc:0.66282\n",
      "alpha:0.75,T:10,acc:0.64434\n",
      "alpha:0.80,T:01,acc:0.67206\n",
      "alpha:0.80,T:02,acc:0.59584\n",
      "alpha:0.80,T:03,acc:0.65820\n",
      "alpha:0.80,T:04,acc:0.58199\n",
      "alpha:0.80,T:05,acc:0.63972\n",
      "alpha:0.80,T:06,acc:0.63510\n",
      "alpha:0.80,T:07,acc:0.63510\n",
      "alpha:0.80,T:08,acc:0.64203\n",
      "alpha:0.80,T:09,acc:0.62125\n",
      "alpha:0.80,T:10,acc:0.61663\n",
      "alpha:0.90,T:01,acc:0.63048\n",
      "alpha:0.90,T:02,acc:0.65127\n",
      "alpha:0.90,T:03,acc:0.72286\n",
      "alpha:0.90,T:04,acc:0.63510\n",
      "alpha:0.90,T:05,acc:0.62125\n",
      "alpha:0.90,T:06,acc:0.64896\n",
      "alpha:0.90,T:07,acc:0.64665\n",
      "alpha:0.90,T:08,acc:0.63279\n",
      "alpha:0.90,T:09,acc:0.68129\n",
      "alpha:0.90,T:10,acc:0.64203\n",
      "alpha:0.95,T:01,acc:0.67898\n",
      "alpha:0.95,T:02,acc:0.60046\n",
      "alpha:0.95,T:03,acc:0.60046\n",
      "alpha:0.95,T:04,acc:0.64665\n",
      "alpha:0.95,T:05,acc:0.64434\n",
      "alpha:0.95,T:06,acc:0.63048\n",
      "alpha:0.95,T:07,acc:0.65820\n",
      "alpha:0.95,T:08,acc:0.62356\n",
      "alpha:0.95,T:09,acc:0.58661\n",
      "alpha:0.95,T:10,acc:0.63279\n",
      "alpha:1.00,T:01,acc:0.68129\n",
      "alpha:1.00,T:02,acc:0.65127\n",
      "alpha:1.00,T:03,acc:0.60739\n",
      "alpha:1.00,T:04,acc:0.64896\n",
      "alpha:1.00,T:05,acc:0.65589\n",
      "alpha:1.00,T:06,acc:0.62587\n",
      "alpha:1.00,T:07,acc:0.66975\n",
      "alpha:1.00,T:08,acc:0.64896\n",
      "alpha:1.00,T:09,acc:0.63972\n",
      "alpha:1.00,T:10,acc:0.63510\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_trainval_tensor=torch.from_numpy(x_trainval).to(torch.float32)\n",
    "y_trainval_onehot_tensor=torch.from_numpy(y_trainval_onehot).to(torch.float32)\n",
    "y_trainval_tensor=torch.from_numpy(y_trainval_num)\n",
    "\n",
    "\n",
    "# alpha=0.8\n",
    "# T=2\n",
    "best_score=0.0\n",
    "best_alpha=0\n",
    "best_T=0\n",
    "for alpha in [0.05,0.1,0.2,0.25,0.5,0.75,0.8,0.9,0.95,1]:\n",
    "    for T in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        nn_clf=nn_student(34,32,14)\n",
    "        optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.01)\n",
    "        loss_fn1=torch.nn.CrossEntropyLoss()\n",
    "        loss_fn2=torch.nn.KLDivLoss()\n",
    "        for i in range(100000):\n",
    "            acc=0.0\n",
    "            prediction=nn_clf(x_trainval_tensor)\n",
    "            loss1=loss_fn1(prediction,y_trainval_tensor)\n",
    "            \n",
    "            output_student=F.log_softmax(prediction/T,dim=1).to(torch.float32)\n",
    "            # output_teacher=F.softmax(y_trainval_predprob/T,dim=1).to(torch.float32)\n",
    "            output_teacher=y_trainval_predprob.to(torch.float32)\n",
    "            loss2=loss_fn2(output_student,output_teacher)*T*T\n",
    "            optimizer.zero_grad()\n",
    "            loss=loss1*(1-alpha)+loss2*alpha\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if i%1000==0:\n",
    "            #     acc += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "            #             np.argmax(y_trainval_onehot,axis=1))\n",
    "            #     print('acc:%.5f'%(acc/len(y_trainval_onehot)))\n",
    "            #     print('[%d, %5d] loss: %.4f loss1: %.4f loss2: %.4f' %(i, 100000, loss.item(), loss1.item(), loss2.item()))\n",
    "\n",
    "        x_test_tensor=torch.from_numpy(x_test).to(torch.float32)\n",
    "        y_test_onehot_tensor=torch.from_numpy(y_test_onehot).to(torch.float32)\n",
    "        test_pred=nn_clf(x_test_tensor)\n",
    "        acc=0.0\n",
    "        acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                        np.argmax(y_test_onehot,axis=1))\n",
    "        acc=acc/len(y_test_onehot)\n",
    "        if acc>best_score:\n",
    "            best_score=acc\n",
    "            best_alpha=alpha\n",
    "            best_T=T\n",
    "        print('alpha:%.2f,T:%02d,acc:%.5f'%(alpha,T,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc:0.10732\n",
      "[0, 100000] loss: 1.4579 loss1: 2.7084 loss2: 1.3189\n",
      "acc:0.45135\n",
      "[1000, 100000] loss: 1.0761 loss1: 1.7544 loss2: 1.0008\n",
      "acc:0.49549\n",
      "[2000, 100000] loss: 0.9243 loss1: 1.5529 loss2: 0.8544\n",
      "acc:0.53360\n",
      "[3000, 100000] loss: 0.8311 loss1: 1.4437 loss2: 0.7630\n",
      "acc:0.54463\n",
      "[4000, 100000] loss: 0.7625 loss1: 1.3570 loss2: 0.6965\n",
      "acc:0.55567\n",
      "[5000, 100000] loss: 0.7091 loss1: 1.2916 loss2: 0.6443\n",
      "acc:0.58576\n",
      "[6000, 100000] loss: 0.6662 loss1: 1.2404 loss2: 0.6024\n",
      "acc:0.59378\n",
      "[7000, 100000] loss: 0.6315 loss1: 1.1996 loss2: 0.5684\n",
      "acc:0.61384\n",
      "[8000, 100000] loss: 0.6028 loss1: 1.1694 loss2: 0.5399\n",
      "acc:0.64193\n",
      "[9000, 100000] loss: 0.5789 loss1: 1.1431 loss2: 0.5163\n",
      "acc:0.65697\n",
      "[10000, 100000] loss: 0.5573 loss1: 1.1094 loss2: 0.4960\n",
      "acc:0.67101\n",
      "[11000, 100000] loss: 0.5378 loss1: 1.0843 loss2: 0.4771\n",
      "acc:0.67904\n",
      "[12000, 100000] loss: 0.5184 loss1: 1.0519 loss2: 0.4591\n",
      "acc:0.68205\n",
      "[13000, 100000] loss: 0.4990 loss1: 1.0122 loss2: 0.4420\n",
      "acc:0.70010\n",
      "[14000, 100000] loss: 0.4816 loss1: 0.9777 loss2: 0.4264\n",
      "acc:0.70812\n",
      "[15000, 100000] loss: 0.4650 loss1: 0.9489 loss2: 0.4112\n",
      "acc:0.70913\n",
      "[16000, 100000] loss: 0.4492 loss1: 0.9237 loss2: 0.3965\n",
      "acc:0.72317\n",
      "[17000, 100000] loss: 0.4385 loss1: 0.9046 loss2: 0.3867\n",
      "acc:0.72417\n",
      "[18000, 100000] loss: 0.4236 loss1: 0.8809 loss2: 0.3728\n",
      "acc:0.73220\n",
      "[19000, 100000] loss: 0.4139 loss1: 0.8633 loss2: 0.3640\n",
      "acc:0.73320\n",
      "[20000, 100000] loss: 0.4014 loss1: 0.8441 loss2: 0.3523\n",
      "acc:0.73621\n",
      "[21000, 100000] loss: 0.3923 loss1: 0.8305 loss2: 0.3436\n",
      "acc:0.73521\n",
      "[22000, 100000] loss: 0.3813 loss1: 0.8105 loss2: 0.3336\n",
      "acc:0.74022\n",
      "[23000, 100000] loss: 0.3723 loss1: 0.7930 loss2: 0.3256\n",
      "acc:0.74122\n",
      "[24000, 100000] loss: 0.3645 loss1: 0.7817 loss2: 0.3182\n",
      "acc:0.74624\n",
      "[25000, 100000] loss: 0.3565 loss1: 0.7674 loss2: 0.3108\n",
      "acc:0.74624\n",
      "[26000, 100000] loss: 0.3490 loss1: 0.7528 loss2: 0.3042\n",
      "acc:0.74524\n",
      "[27000, 100000] loss: 0.3424 loss1: 0.7427 loss2: 0.2979\n",
      "acc:0.74524\n",
      "[28000, 100000] loss: 0.3358 loss1: 0.7306 loss2: 0.2920\n",
      "acc:0.74624\n",
      "[29000, 100000] loss: 0.3299 loss1: 0.7205 loss2: 0.2865\n",
      "acc:0.76229\n",
      "[30000, 100000] loss: 0.3243 loss1: 0.7114 loss2: 0.2813\n",
      "acc:0.77232\n",
      "[31000, 100000] loss: 0.3186 loss1: 0.7023 loss2: 0.2760\n",
      "acc:0.77633\n",
      "[32000, 100000] loss: 0.3135 loss1: 0.6925 loss2: 0.2713\n",
      "acc:0.78134\n",
      "[33000, 100000] loss: 0.3074 loss1: 0.6775 loss2: 0.2663\n",
      "acc:0.78335\n",
      "[34000, 100000] loss: 0.3008 loss1: 0.6585 loss2: 0.2611\n",
      "acc:0.78736\n",
      "[35000, 100000] loss: 0.2949 loss1: 0.6443 loss2: 0.2560\n",
      "acc:0.79539\n",
      "[36000, 100000] loss: 0.2903 loss1: 0.6361 loss2: 0.2519\n",
      "acc:0.79840\n",
      "[37000, 100000] loss: 0.2860 loss1: 0.6268 loss2: 0.2481\n",
      "acc:0.80040\n",
      "[38000, 100000] loss: 0.2810 loss1: 0.6137 loss2: 0.2440\n",
      "acc:0.80241\n",
      "[39000, 100000] loss: 0.2770 loss1: 0.6059 loss2: 0.2405\n",
      "acc:0.80341\n",
      "[40000, 100000] loss: 0.2733 loss1: 0.5985 loss2: 0.2371\n",
      "acc:0.80642\n",
      "[41000, 100000] loss: 0.2696 loss1: 0.5913 loss2: 0.2339\n",
      "acc:0.80742\n",
      "[42000, 100000] loss: 0.2661 loss1: 0.5843 loss2: 0.2308\n",
      "acc:0.80742\n",
      "[43000, 100000] loss: 0.2628 loss1: 0.5776 loss2: 0.2278\n",
      "acc:0.80742\n",
      "[44000, 100000] loss: 0.2595 loss1: 0.5706 loss2: 0.2249\n",
      "acc:0.80642\n",
      "[45000, 100000] loss: 0.2563 loss1: 0.5642 loss2: 0.2221\n",
      "acc:0.80642\n",
      "[46000, 100000] loss: 0.2533 loss1: 0.5578 loss2: 0.2195\n",
      "acc:0.80742\n",
      "[47000, 100000] loss: 0.2504 loss1: 0.5515 loss2: 0.2169\n",
      "acc:0.81846\n",
      "[48000, 100000] loss: 0.2475 loss1: 0.5453 loss2: 0.2145\n",
      "acc:0.83551\n",
      "[49000, 100000] loss: 0.2447 loss1: 0.5388 loss2: 0.2120\n",
      "acc:0.83952\n",
      "[50000, 100000] loss: 0.2420 loss1: 0.5331 loss2: 0.2097\n",
      "acc:0.84253\n",
      "[51000, 100000] loss: 0.2393 loss1: 0.5263 loss2: 0.2074\n",
      "acc:0.84554\n",
      "[52000, 100000] loss: 0.2368 loss1: 0.5207 loss2: 0.2052\n",
      "acc:0.84855\n",
      "[53000, 100000] loss: 0.2343 loss1: 0.5147 loss2: 0.2031\n",
      "acc:0.84955\n",
      "[54000, 100000] loss: 0.2319 loss1: 0.5096 loss2: 0.2011\n",
      "acc:0.85155\n",
      "[55000, 100000] loss: 0.2297 loss1: 0.5047 loss2: 0.1991\n",
      "acc:0.85055\n",
      "[56000, 100000] loss: 0.2275 loss1: 0.5000 loss2: 0.1972\n",
      "acc:0.85256\n",
      "[57000, 100000] loss: 0.2251 loss1: 0.4945 loss2: 0.1951\n",
      "acc:0.85557\n",
      "[58000, 100000] loss: 0.2230 loss1: 0.4892 loss2: 0.1934\n",
      "acc:0.85657\n",
      "[59000, 100000] loss: 0.2209 loss1: 0.4845 loss2: 0.1916\n",
      "acc:0.85657\n",
      "[60000, 100000] loss: 0.2189 loss1: 0.4800 loss2: 0.1899\n",
      "acc:0.85858\n",
      "[61000, 100000] loss: 0.2169 loss1: 0.4754 loss2: 0.1882\n",
      "acc:0.85858\n",
      "[62000, 100000] loss: 0.2145 loss1: 0.4657 loss2: 0.1866\n",
      "acc:0.85757\n",
      "[63000, 100000] loss: 0.2121 loss1: 0.4617 loss2: 0.1844\n",
      "acc:0.85657\n",
      "[64000, 100000] loss: 0.2100 loss1: 0.4570 loss2: 0.1826\n",
      "acc:0.85657\n",
      "[65000, 100000] loss: 0.2081 loss1: 0.4524 loss2: 0.1809\n",
      "acc:0.86158\n",
      "[66000, 100000] loss: 0.2061 loss1: 0.4470 loss2: 0.1793\n",
      "acc:0.86158\n",
      "[67000, 100000] loss: 0.2042 loss1: 0.4427 loss2: 0.1777\n",
      "acc:0.86158\n",
      "[68000, 100000] loss: 0.2025 loss1: 0.4385 loss2: 0.1762\n",
      "acc:0.86158\n",
      "[69000, 100000] loss: 0.2007 loss1: 0.4344 loss2: 0.1747\n",
      "acc:0.86158\n",
      "[70000, 100000] loss: 0.1989 loss1: 0.4306 loss2: 0.1732\n",
      "acc:0.86158\n",
      "[71000, 100000] loss: 0.1972 loss1: 0.4268 loss2: 0.1717\n",
      "acc:0.86158\n",
      "[72000, 100000] loss: 0.1956 loss1: 0.4231 loss2: 0.1703\n",
      "acc:0.86158\n",
      "[73000, 100000] loss: 0.1941 loss1: 0.4195 loss2: 0.1690\n",
      "acc:0.86158\n",
      "[74000, 100000] loss: 0.1926 loss1: 0.4161 loss2: 0.1677\n",
      "acc:0.86158\n",
      "[75000, 100000] loss: 0.1911 loss1: 0.4128 loss2: 0.1665\n",
      "acc:0.86359\n",
      "[76000, 100000] loss: 0.1897 loss1: 0.4096 loss2: 0.1653\n",
      "acc:0.86359\n",
      "[77000, 100000] loss: 0.1883 loss1: 0.4066 loss2: 0.1641\n",
      "acc:0.86359\n",
      "[78000, 100000] loss: 0.1870 loss1: 0.4036 loss2: 0.1630\n",
      "acc:0.86359\n",
      "[79000, 100000] loss: 0.1857 loss1: 0.4001 loss2: 0.1619\n",
      "acc:0.86760\n",
      "[80000, 100000] loss: 0.1841 loss1: 0.3953 loss2: 0.1607\n",
      "acc:0.86760\n",
      "[81000, 100000] loss: 0.1828 loss1: 0.3922 loss2: 0.1595\n",
      "acc:0.86760\n",
      "[82000, 100000] loss: 0.1815 loss1: 0.3895 loss2: 0.1584\n",
      "acc:0.87161\n",
      "[83000, 100000] loss: 0.1802 loss1: 0.3866 loss2: 0.1572\n",
      "acc:0.87262\n",
      "[84000, 100000] loss: 0.1789 loss1: 0.3840 loss2: 0.1561\n",
      "acc:0.87061\n",
      "[85000, 100000] loss: 0.1777 loss1: 0.3818 loss2: 0.1550\n",
      "acc:0.87663\n",
      "[86000, 100000] loss: 0.1765 loss1: 0.3794 loss2: 0.1539\n",
      "acc:0.85456\n",
      "[87000, 100000] loss: 0.1903 loss1: 0.4216 loss2: 0.1645\n",
      "acc:0.87763\n",
      "[88000, 100000] loss: 0.1734 loss1: 0.3689 loss2: 0.1517\n",
      "acc:0.88265\n",
      "[89000, 100000] loss: 0.1676 loss1: 0.3462 loss2: 0.1478\n",
      "acc:0.89769\n",
      "[90000, 100000] loss: 0.1659 loss1: 0.3436 loss2: 0.1461\n",
      "acc:0.89769\n",
      "[91000, 100000] loss: 0.1646 loss1: 0.3413 loss2: 0.1450\n",
      "acc:0.89870\n",
      "[92000, 100000] loss: 0.1633 loss1: 0.3383 loss2: 0.1439\n",
      "acc:0.89970\n",
      "[93000, 100000] loss: 0.1611 loss1: 0.3332 loss2: 0.1420\n",
      "acc:0.89970\n",
      "[94000, 100000] loss: 0.1599 loss1: 0.3305 loss2: 0.1409\n",
      "acc:0.89970\n",
      "[95000, 100000] loss: 0.1590 loss1: 0.3283 loss2: 0.1402\n",
      "acc:0.89970\n",
      "[96000, 100000] loss: 0.1579 loss1: 0.3264 loss2: 0.1392\n",
      "acc:0.90070\n",
      "[97000, 100000] loss: 0.1566 loss1: 0.3225 loss2: 0.1382\n",
      "acc:0.90572\n",
      "[98000, 100000] loss: 0.1540 loss1: 0.3133 loss2: 0.1363\n",
      "acc:0.90572\n",
      "[99000, 100000] loss: 0.1526 loss1: 0.3106 loss2: 0.1350\n",
      "alpha:0.90,T:03,acc:0.66975\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class nn_student(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super(nn_student,self).__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x_trainval_tensor=torch.from_numpy(x_trainval).to(torch.float32)\n",
    "y_trainval_onehot_tensor=torch.from_numpy(y_trainval_onehot).to(torch.float32)\n",
    "y_trainval_tensor=torch.from_numpy(y_trainval_num)\n",
    "\n",
    "\n",
    "alpha=0.9\n",
    "T=3\n",
    "\n",
    "nn_clf=nn_student(34,32,14)\n",
    "optimizer=torch.optim.SGD(nn_clf.parameters(),lr=0.01)\n",
    "loss_fn1=torch.nn.CrossEntropyLoss()\n",
    "loss_fn2=torch.nn.KLDivLoss()\n",
    "for i in range(100000):\n",
    "    acc=0.0\n",
    "    prediction=nn_clf(x_trainval_tensor)\n",
    "    loss1=loss_fn1(prediction,y_trainval_tensor)\n",
    "    \n",
    "    output_student=F.log_softmax(prediction/T,dim=1).to(torch.float32)\n",
    "    # output_teacher=F.softmax(y_trainval_predprob/T,dim=1).to(torch.float32)\n",
    "    output_teacher=y_trainval_predprob.to(torch.float32)\n",
    "    loss2=loss_fn2(output_student,output_teacher)*T*T\n",
    "    optimizer.zero_grad()\n",
    "    loss=loss1*(1-alpha)+loss2*alpha\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        acc += np.sum(np.argmax(prediction.cpu().data.numpy(), axis=1) ==\n",
    "                np.argmax(y_trainval_onehot,axis=1))\n",
    "        print('acc:%.5f'%(acc/len(y_trainval_onehot)))\n",
    "        print('[%d, %5d] loss: %.4f loss1: %.4f loss2: %.4f' %(i, 100000, loss.item(), loss1.item(), loss2.item()))\n",
    "\n",
    "x_test_tensor=torch.from_numpy(x_test).to(torch.float32)\n",
    "y_test_onehot_tensor=torch.from_numpy(y_test_onehot).to(torch.float32)\n",
    "test_pred=nn_clf(x_test_tensor)\n",
    "acc=0.0\n",
    "acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) ==\n",
    "                np.argmax(y_test_onehot,axis=1))\n",
    "acc=acc/len(y_test_onehot)\n",
    "\n",
    "print('alpha:%.2f,T:%02d,acc:%.5f'%(alpha,T,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}